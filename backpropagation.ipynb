{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTOtUK23SYJT58FB4saOVW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apurvakumbhar/ANN/blob/main/backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS3FdvFRMuIE",
        "outputId": "1b83b34b-18fd-4681-e9d1-5801ae63a5f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Error: 0.3017354733885044\n",
            "Epoch 1000, Error: 0.0013178877888345977\n",
            "Epoch 2000, Error: 0.00019974009527213053\n",
            "Epoch 3000, Error: 7.383347718665172e-05\n",
            "Epoch 4000, Error: 3.7572188555925626e-05\n",
            "Epoch 5000, Error: 2.2541803917863946e-05\n",
            "Epoch 6000, Error: 1.4951897756165067e-05\n",
            "Epoch 7000, Error: 1.0610510473336771e-05\n",
            "Epoch 8000, Error: 7.904170633600707e-06\n",
            "Epoch 9000, Error: 6.107345366516064e-06\n",
            "Final prediction after training: [0.00269533 0.99843531]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "# Sigmoid derivative\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Initialize the network parameters\n",
        "# Inputs\n",
        "x1 = np.array([0, 1])\n",
        "x2 = np.array([0, 1])\n",
        "\n",
        "# Weights (randomly initialized)\n",
        "w1 = np.random.randn()\n",
        "w2 = np.random.randn()\n",
        "w3 = np.random.randn()\n",
        "w4 = np.random.randn()\n",
        "\n",
        "# Bias terms (randomly initialized)\n",
        "b1 = np.random.randn()\n",
        "b2 = np.random.randn()\n",
        "\n",
        "# Actual output for calculating error\n",
        "y_actual = np.array([0, 1])\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Number of epochs\n",
        "epochs = 10000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    h = sigmoid(w1*x1 + w2*x2 + b1)  # Hidden layer output\n",
        "    y_pred = sigmoid(w3*h + w4*h + b2)  # Output layer prediction\n",
        "\n",
        "    # Calculate the error\n",
        "    error = 0.5 * np.sum((y_actual - y_pred)**2)\n",
        "\n",
        "    # Backward pass\n",
        "    # Compute gradients for each weight using the chain rule\n",
        "    dE_dypred = y_pred - y_actual\n",
        "    dypred_dw3 = h\n",
        "    dypred_dh = w3 * sigmoid_derivative(h) + w4 * sigmoid_derivative(h)  # derivative of output layer with respect to hidden layer\n",
        "    dE_dh = dE_dypred * dypred_dh\n",
        "\n",
        "    # Gradients for weights and bias\n",
        "    dE_dw3 = np.sum(dE_dypred * dypred_dw3)\n",
        "    dE_dw4 = np.sum(dE_dypred * dypred_dw3)\n",
        "    dE_db2 = np.sum(dE_dypred)\n",
        "\n",
        "    dE_dh = dE_dypred * sigmoid_derivative(h)\n",
        "    dE_dw1 = np.sum(dE_dh * x1)\n",
        "    dE_dw2 = np.sum(dE_dh * x2)\n",
        "    dE_db1 = np.sum(dE_dh)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * dE_dw1\n",
        "    w2 -= learning_rate * dE_dw2\n",
        "    w3 -= learning_rate * dE_dw3\n",
        "    w4 -= learning_rate * dE_dw4\n",
        "    b1 -= learning_rate * dE_db1\n",
        "    b2 -= learning_rate * dE_db2\n",
        "\n",
        "    # Print the error every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Error: {error}\")\n",
        "\n",
        "# Final prediction after training\n",
        "print(\"Final prediction after training:\", y_pred)\n"
      ]
    }
  ]
}